{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial Tabular Q Learning.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"3ZG9J_84NQRb","colab_type":"text"},"cell_type":"markdown","source":["## One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning (Watkins, 1989)"]},{"metadata":{"id":"wLtcxjwToUYX","colab_type":"text"},"cell_type":"markdown","source":["##  Installation of required tools -\n","We will use Open AI's gym environment for this tutorial."]},{"metadata":{"id":"W-3fTWvroD5M","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":472},"outputId":"2710de01-cd42-4351-994c-626fb7d3b666","executionInfo":{"status":"ok","timestamp":1527269355800,"user_tz":240,"elapsed":6623,"user":{"displayName":"Airi C.","photoUrl":"//lh6.googleusercontent.com/-mRsT-p21xZI/AAAAAAAAAAI/AAAAAAAAHeU/RHc7qaRZabg/s50-c-k-no/photo.jpg","userId":"109705951704145205723"}}},"cell_type":"code","source":["! pip install git+https://github.com/openai/gym.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/openai/gym.git\r\n","  Cloning https://github.com/openai/gym.git to /tmp/pip-req-build-WIMcJ0\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python2.7/dist-packages (from gym==0.10.5) (1.14.3)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python2.7/dist-packages (from gym==0.10.5) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from gym==0.10.5) (1.11.0)\n","Collecting pyglet>=1.2.0 (from gym==0.10.5)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n","\u001b[K    100% |████████████████████████████████| 1.0MB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym==0.10.5) (2018.4.16)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n","Building wheels for collected packages: gym\n","  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-a084EA/wheels/13/4b/2f/79d47b11ac3a37fc33b74d4bdf9031be155b70ff8b7ca24571\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","Successfully installed gym-0.10.5 pyglet-1.3.2\n"],"name":"stdout"}]},{"metadata":{"id":"0_4GQydcrr4p","colab_type":"text"},"cell_type":"markdown","source":["## Task Description"]},{"metadata":{"id":"2apV1etj6PX2","colab_type":"text"},"cell_type":"markdown","source":["Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. \n","\n","The water is mostly frozen, but there are a **few holes** where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend."]},{"metadata":{"id":"JRZdzR1qoQrH","colab_type":"text"},"cell_type":"markdown","source":["Consider a **2-D grid world**. The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.  Let us visualize this environment https://gym.openai.com/envs/FrozenLake-v0/"]},{"metadata":{"id":"HIHEobVQ6gX5","colab_type":"text"},"cell_type":"markdown","source":["SFFF       (S: starting point, safe)\n","\n","FHFH       (F: frozen surface, safe)\n","\n","FFFH       (H: hole, fall to your doom)\n","\n","HFFG       (G: goal, where the frisbee is located)"]},{"metadata":{"id":"GiF5imrmr1Rg","colab_type":"text"},"cell_type":"markdown","source":["## Environments "]},{"metadata":{"id":"2BtG5D_GoxHN","colab_type":"text"},"cell_type":"markdown","source":["Where do these environments come from ? \n","- [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\n","- [ Open AI Gym](https://gym.openai.com/)\n","- [Deep Mind Lab](https://github.com/deepmind/lab)"]},{"metadata":{"id":"WaikKR9ypUIt","colab_type":"text"},"cell_type":"markdown","source":["**Problem solving approach in RL**\n","\n","* Task : what do you want to solve ? \n","* Environment : determinstic or stochastic \n","* Transition dynamics : P(s' | s ),   R( s, a)\n","\n","\n"]},{"metadata":{"id":"SkvzpAqfrBc6","colab_type":"text"},"cell_type":"markdown","source":["## Understanding what is the action space and  observation space"]},{"metadata":{"id":"5co6zjGfrRX2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":50},"outputId":"f172fc92-0993-49c0-daf3-e655bf30d9df","executionInfo":{"status":"ok","timestamp":1527262240127,"user_tz":240,"elapsed":265,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","env = gym.make('FrozenLake-v0')\n","\n","print(env.action_space)\n","\n","print(env.observation_space)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Discrete(4)\n","Discrete(16)\n"],"name":"stdout"}]},{"metadata":{"id":"CYQFKWoD7oTE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":101},"outputId":"e7fcfb3f-2cf9-4ed2-90ba-e12d62e5173e","executionInfo":{"status":"ok","timestamp":1527262385625,"user_tz":240,"elapsed":259,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["env.render()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"metadata":{"id":"-nfwY460_FSr","colab_type":"text"},"cell_type":"markdown","source":["Notice that \n","\n","\n","*   0 Left \n","*   1 Down\n","*   2 Right \n","*   3 Up\n"]},{"metadata":{"id":"nn_dX0LC74bG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":101},"outputId":"8fe4536f-da5d-48f2-b2b6-ee02e927abcd","executionInfo":{"status":"ok","timestamp":1527263337276,"user_tz":240,"elapsed":448,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["action_set = [0, 1, 2, 3]\n","env.reset()\n","a,b,c,d = env.step(3)\n","env.render()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n"],"name":"stdout"}]},{"metadata":{"id":"REPwKHGa_Z8w","colab_type":"text"},"cell_type":"markdown","source":["## The environment’s step function"]},{"metadata":{"id":"7uxEPjQO_VpN","colab_type":"text"},"cell_type":"markdown","source":["Taking a step returns four values. These are:\n","\n","* **observation (object)**: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n","\n","\n","* **reward (float)**: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n","\n","* **done (boolean)**: whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n","\n","* **info (dict)**: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning."]},{"metadata":{"id":"zDPlrg_g7H7d","colab_type":"text"},"cell_type":"markdown","source":["# Sample Agent Environment Interface "]},{"metadata":{"id":"XEezNWS967HB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":655},"outputId":"cfd2021e-2534-4f14-d983-3c10434b69d2","executionInfo":{"status":"ok","timestamp":1527263345754,"user_tz":240,"elapsed":255,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["for i_episode in range(2):\n","    observation = env.reset()\n","    for t in range(10):\n","        env.render()\n","        print(observation)\n","        action = env.action_space.sample()\n","        observation, reward, done, info = env.step(action)\n","        if done:\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 4 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 2 timesteps\n"],"name":"stdout"}]},{"metadata":{"id":"AIEWvjbIuLTw","colab_type":"text"},"cell_type":"markdown","source":["## Epsilon Greedy Function \n","\n","    - Chooses a greedy action most of the time but with a probability eps chooses a random action\n","    - Chooses random action with probability of eps; argmax Q(s, .) with probability of (1-eps)"]},{"metadata":{"id":"6JW8Oqi1tkp1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def eps_greedy(q_vals, eps, state):\n","    \"\"\"\n","    Inputs:\n","        q_vals: q value tables\n","        eps: epsilon\n","        state: current state\n","    Outputs:\n","        random action with probability of eps; argmax Q(s, .) with probability of (1-eps)\n","    \"\"\"\n","    import random\n","    if random.random() <= eps:\n","        action = env.action_space.sample() # sample an action randomly # sample an action randomly\n","    else:\n","        action = np.argmax(q_vals[state,:])\n","    return action"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I-Uuq9xbuXWP","colab_type":"text"},"cell_type":"markdown","source":["## Q learning update function. "]},{"metadata":{"id":"updgrX-z2C4g","colab_type":"text"},"cell_type":"markdown","source":["* Q learning update function. After we observe a transition $s, a, s', r$,\n","     \n","     $$\\textrm{target}(s') = R(s,a,s') + \\gamma \\max_{a'} Q_{\\theta_k}(s',a')$$\n","     $$\\textrm{delta}(s') = \\textrm{target}(s') - Q_{\\theta_k}(s',a')\n","     $$$$Q_{k+1}(s,a) \\leftarrow Q_k(s,a) + \\alpha * \\left( \\textrm{delta}(s') \\right)$$\n"]},{"metadata":{"id":"lZFRGMEOtvi-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward):\n","    \"\"\"\n","    Inputs:\n","        gamma: discount factor\n","        alpha: learning rate\n","        q_vals: q value table\n","        cur_state: current state\n","        action: action taken in current state\n","        next_state: next state results from taking `action` in `cur_state`\n","        reward: reward received from this transition\n","    \n","    Performs in-place update of q_vals table to implement one step of Q-learning\n","    \"\"\"\n","    delta = reward + gamma * np.max(q_vals[next_state,:]) - q_vals[cur_state,action]\n","    q_vals[cur_state,action] = q_vals[cur_state,action] + alpha * delta"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uYRZkoF22JVj","colab_type":"text"},"cell_type":"markdown","source":["## Algorithm"]},{"metadata":{"id":"nyKYImxpFRHe","colab_type":"text"},"cell_type":"markdown","source":["**Let us first see what happens when we always take Greedy Action**"]},{"metadata":{"id":"II2Pi61OFK_C","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":302},"outputId":"86ae872a-c6fd-4ca7-c000-cb972ebb6369","executionInfo":{"status":"ok","timestamp":1527265166709,"user_tz":240,"elapsed":840,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["env = gym.make('FrozenLake-v0')\n","#env = gym.make('FrozenLakeNotSlippery18x18-v0')\n","\n","Q = np.zeros([env.observation_space.n,env.action_space.n])\n","gamma = 0.95\n","alpha = 0.8\n","epsilon = 0.1\n","episodes_num = 2000\n","rList = []\n","for itr in range(episodes_num):\n","    cur_state = env.reset()\n","    ret = 0\n","    done = False\n","    while not done:\n","        action = eps_greedy(Q, epsilon, cur_state)\n","        next_state, reward, done, info = env.step(action)\n","        q_learning_update(gamma, alpha, Q, cur_state, action, next_state, reward)\n","        cur_state = next_state\n","        ret+=reward\n","    rList.append(ret)\n","print (\"Score over time: \" +  str(sum(rList)/episodes_num))\n","print(\"Q-values: %s\" %Q)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Score over time: 0.0\n","Q-values: [[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"-fB7IO5wFdAx","colab_type":"text"},"cell_type":"markdown","source":["**Do something else**"]},{"metadata":{"id":"9OODtcb0t1dd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":302},"outputId":"26718b99-e5d0-4a53-b746-7f26b9d56cc4","executionInfo":{"status":"ok","timestamp":1527266555319,"user_tz":240,"elapsed":2121,"user":{"displayName":"Khimya Khetarpal","photoUrl":"//lh4.googleusercontent.com/-N2vm2KOF7iE/AAAAAAAAAAI/AAAAAAAACL8/3K7KcnaQmY0/s50-c-k-no/photo.jpg","userId":"111336538593611088506"}}},"cell_type":"code","source":["np.set_printoptions(formatter={'float': '{: 0.5f}'.format})\n","env = gym.make('FrozenLake-v0')\n","\n","Q = np.zeros([env.observation_space.n,env.action_space.n])\n","gamma = 0.95\n","alpha = 0.8\n","epsilon = 0.1\n","episodes_num = 2000\n","rList = []\n","for itr in range(episodes_num):\n","    cur_state = env.reset()\n","    ret = 0\n","    done = False\n","    while not done:\n","        #action = eps_greedy(Q, epsilon, cur_state)\n","        #print(action)\n","        action = np.argmax(Q[cur_state,:] + np.random.randn(1,env.action_space.n)*(1./(itr+1)))\n","        next_state, reward, done, info = env.step(action)\n","        q_learning_update(gamma, alpha, Q, cur_state, action, next_state, reward)\n","        #Q[cur_state,action] = Q[cur_state,action] + alpha*(reward + gamma*np.max(Q[next_state,:]) - Q[cur_state,action])\n","        cur_state = next_state\n","        ret+=reward\n","    rList.append(ret)\n","    #epsilon = max(epsilon-0.002,0.1)\n","print (\"Score over time: \" +  str(sum(rList)/episodes_num))\n","print(\"Q-values:\", Q)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Score over time: 0.467\n","('Q-values:', array([[ 0.00444,  0.00360,  0.31380,  0.00359],\n","       [ 0.00210,  0.00190,  0.00199,  0.05323],\n","       [ 0.00224,  0.00055,  0.00000,  0.01492],\n","       [ 0.00040,  0.00026,  0.00007,  0.00768],\n","       [ 0.42539,  0.00135,  0.00047,  0.00134],\n","       [ 0.00000,  0.00000,  0.00000,  0.00000],\n","       [ 0.00001,  0.00033,  0.14166,  0.00002],\n","       [ 0.00000,  0.00000,  0.00000,  0.00000],\n","       [ 0.00168,  0.00115,  0.00188,  0.43573],\n","       [ 0.00183,  0.52828,  0.00172,  0.00289],\n","       [ 0.57150,  0.00129,  0.00023,  0.00006],\n","       [ 0.00000,  0.00000,  0.00000,  0.00000],\n","       [ 0.00000,  0.00000,  0.00000,  0.00000],\n","       [ 0.00187,  0.00000,  0.83845,  0.00000],\n","       [ 0.00000,  0.00000,  0.00000,  0.99212],\n","       [ 0.00000,  0.00000,  0.00000,  0.00000]]))\n"],"name":"stdout"}]}]}